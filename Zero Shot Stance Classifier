# Environment-aware pip install
try:
    import google.colab
    IN_COLAB = True
except ImportError:
    IN_COLAB = False

if IN_COLAB:
    # Only install if running in Colab
    import sys
    !{sys.executable} -m pip install transformers pandas --quiet

import pandas as pd
import re
from typing import Dict, List
from transformers import pipeline

# -----------------------------
# Step 1: Load data
# -----------------------------
# Make sure to upload `cleaned_tweets.jsonl` using the file upload tool in Colab
df = pd.read_json("cleaned_tweets.jsonl", lines=True)

# -----------------------------
# Step 2: Define alias mapping
# -----------------------------
ALIAS2CANONICAL: Dict[str, str] = {
    # Luigi Mangione
    "luigi": "Luigi Mangione",
    "mangione": "Luigi Mangione",
    "shooter": "Luigi Mangione",
    "assassin": "Luigi Mangione",
    # United Healthcare
    "unitedhealth": "United Healthcare",
    "uhc": "United Healthcare",
    "healthcare": "United Healthcare",
    "corporati": "United Healthcare",
    "conglomerate": "United Healthcare",
    "insurance": "United Healthcare",
    "company": "United Healthcare",
    # Brian Thompson
    "ceo": "Brian Thompson",
    "@brianthompson": "Brian Thompson",
    "thompson": "Brian Thompson",
    "billionaire": "Brian Thompson",
    "exec": "Brian Thompson",
}
CANONICALS = set(ALIAS2CANONICAL.values())

def extract_entities(text: str) -> List[str]:
    """
    Extract canonical entity names from the given text based on alias mapping,
    and also detect canonical names directly in the text.
    Returns a list of canonical entity names found in the text.
    """
    found = set()
    lowered = text.lower()
    for alias, canonical in ALIAS2CANONICAL.items():
        if re.search(r'\b' + re.escape(alias) + r'\b', lowered):
            found.add(canonical)
    for canonical in CANONICALS:
        if canonical.lower() in lowered:
            found.add(canonical)
    return list(found)

# -----------------------------
# Step 3: Duplicate tweets per entity
# -----------------------------
expanded_rows = []

for _, row in df.iterrows():
    entities = extract_entities(row["text"])
    if not entities:
        # Optionally, include tweets with no detected entity
        expanded_rows.append({
            "original_text": row["text"],
            "entity": "None",
            "text_with_entity": row["text"]
        })
        continue
    for entity in entities:
        # Highlight only the first occurrence of the entity in the text, if present
        pattern = re.compile(re.escape(entity), re.IGNORECASE)
        if pattern.search(row["text"]):
            text_with_entity = pattern.sub(f"</s>{entity}</s>", row["text"], count=1)
        else:
            text_with_entity = f"</s>{entity}</s> {row['text']}"
        expanded_rows.append({
            "original_text": row["text"],
            "entity": entity,
            "text_with_entity": text_with_entity
        })

expanded_df = pd.DataFrame(expanded_rows)

# -----------------------------
# Step 4: Zero-shot stance prediction (BART-MNLI)
# -----------------------------
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

def get_hypotheses(entity):
    """
    Return hypotheses for zero-shot stance classification for a given entity.
    """
    return {
        "favor": f"{entity} is good.",
        "against": f"{entity} is bad.",
        "neutral": f"{entity} exists."
    }

def batch_predict(texts: List[str], entities: List[str], batch_size: int = 16) -> List[str]:
    """
    Batch predict stances for a list of texts and corresponding entities.
    Returns a list of predicted stances ("favor", "against", "neutral").
    """
    results = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        batch_entities = entities[i:i + batch_size]
        batch_hypotheses = [list(get_hypotheses(entity).values()) for entity in batch_entities]
        # The HF pipeline can take a list of candidate_labels for each text
        batch_results = classifier(batch_texts, batch_hypotheses, multi_label=False)
        # If only one sample, HF returns a dict, else a list
        if isinstance(batch_results, dict):
            batch_results = [batch_results]
        for res, entity in zip(batch_results, batch_entities):
            label_map = {v: k for k, v in get_hypotheses(entity).items()}
            results.append(label_map[res["labels"][0]])
    return results

# Run predictions
texts = expanded_df["text_with_entity"].tolist()
entities = expanded_df["entity"].tolist()
expanded_df["predicted_stance"] = batch_predict(texts, entities, batch_size=16)

# -----------------------------
# Step 5: Save results
# -----------------------------
expanded_df.to_csv("zero_shot_stance_predictions.csv", index=False)
print("âœ… Predictions saved to zero_shot_stance_predictions.csv")
